{
 "metadata": {
  "name": "",
  "signature": "sha256:73aff43372b38f9e2749d0b67a15b79561804be6b50ac775806d645642dc3ed5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import sklearn as sk\n",
      "print 'pandas version: ',pd.__version__\n",
      "print 'numpy version:',np.__version__\n",
      "print 'sklearn version:',sk.__version__"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "pandas version:  0.13.1\n",
        "numpy version: 1.8.1\n",
        "sklearn version: 0.14.1\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "home_dir='/home/ubuntu/UCSD_BigData'\n",
      "sys.path.append(home_dir+'/utils')\n",
      "from find_waiting_flow import *\n",
      "from AWS_keypair_management import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#A = AWS_keypair_management()\n",
      "#(Creds,bad_files) = A.Get_Working_Credentials('/home/ubuntu/Vault')\n",
      "#pair = Creds['nima']['Creds'][0]\n",
      "#key_id = pair['Access_Key_Id']\n",
      "#secret_key = pair['Secret_Access_Key']\n",
      "#job_flow_id = find_waiting_flow(key_id,secret_key)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "credentials.csv AWS creds: nima AKIAJ7YQZ7V7CO53WP5A\n",
        "an active key pair"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<boto.emr.emrobject.JobFlow object at 0x4aa2290>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " no_script.yoavfreund.20140516.040032.370095 j-262J0JTFJIRLO WAITING\n",
        "<boto.emr.emrobject.JobFlow object at 0x4c865d0> no_script.yoavfreund.20140517.080731.371759 j-1RE8D7HBISOI0 WAITING\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile mr_weather.py\n",
      "#!/usr/bin/python\n",
      "\"\"\"\n",
      "count the number of measurements of each type\n",
      "\"\"\"\n",
      "import sys\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages')\n",
      "from mrjob.job import MRJob\n",
      "import re\n",
      "from sys import stderr\n",
      "\n",
      "class MRWeather(MRJob):\n",
      "\n",
      "    def mapper(self, _, line):\n",
      "        try:\n",
      "            self.increment_counter('MrJob Counters','mapper-all',1)\n",
      "            elements=line.split(',')\n",
      "            if elements[0]=='station':\n",
      "                out=('header',1)\n",
      "            else:\n",
      "                out=(elements[1],1)\n",
      "        except Exception, e:\n",
      "            stderr.write('Error in line:\\n'+line)\n",
      "            stderr.write(e)\n",
      "            self.increment_counter('MrJob Counters','mapper-error',1)\n",
      "            out=('error',1)\n",
      "\n",
      "        finally:\n",
      "            yield out\n",
      "            \n",
      "    def combiner(self, word, counts):\n",
      "        self.increment_counter('MrJob Counters','combiner',1)\n",
      "        yield (word, sum(counts))\n",
      "        #l_counts=[c for c in counts]  # extract list from iterator\n",
      "        #S=sum(l_counts)\n",
      "        #logfile.write('combiner '+word+' ['+','.join([str(c) for c in l_counts])+']='+str(S)+'\\n')\n",
      "        #yield (word, S)\n",
      "\n",
      "    def reducer(self, word, counts):\n",
      "        self.increment_counter('MrJob Counters','reducer',1)\n",
      "        yield (word, sum(counts))\n",
      "        #l_counts=[c for c in counts]  # extract list from iterator\n",
      "        #S=sum(l_counts)\n",
      "        #logfile.write('reducer '+word+' ['+','.join([str(c) for c in l_counts])+']='+str(S)+'\\n')\n",
      "        #yield (word, S)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    MRWeather.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting mr_weather.py\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Running job inline"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "local_data='/home/ubuntu/UCSD_BigData/data/weather/ALL.head.csv'\n",
      "!ls -l $local_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-rw-rw-r-- 1 ubuntu ubuntu 858960 May 19 22:40 /home/ubuntu/UCSD_BigData/data/weather/ALL.head.csv\r\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python mr_weather.py $local_data > counts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "using configs in /home/ubuntu/.mrjob.conf\r\n",
        "creating tmp directory /tmp/mr_weather.ubuntu.20140520.214212.049854\r\n",
        "writing to /tmp/mr_weather.ubuntu.20140520.214212.049854/step-0-mapper_part-00000\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counters from step 1:\r\n",
        "  MrJob Counters:\r\n",
        "    combiner: 21\r\n",
        "    mapper-all: 999\r\n",
        "writing to /tmp/mr_weather.ubuntu.20140520.214212.049854/step-0-mapper-sorted\r\n",
        "> sort /tmp/mr_weather.ubuntu.20140520.214212.049854/step-0-mapper_part-00000\r\n",
        "writing to /tmp/mr_weather.ubuntu.20140520.214212.049854/step-0-reducer_part-00000\r\n",
        "Counters from step 1:\r\n",
        "  MrJob Counters:\r\n",
        "    combiner: 21\r\n",
        "    mapper-all: 999\r\n",
        "    reducer: 21\r\n",
        "Moving /tmp/mr_weather.ubuntu.20140520.214212.049854/step-0-reducer_part-00000 -> /tmp/mr_weather.ubuntu.20140520.214212.049854/output/part-00000\r\n",
        "Streaming final output from /tmp/mr_weather.ubuntu.20140520.214212.049854/output\r\n",
        "removing tmp directory /tmp/mr_weather.ubuntu.20140520.214212.049854\r\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## managing AWS Credentials ##\n",
      "The full details on how to get the credentials set up is given here: https://docs.google.com/document/d/1xDUy4ZI2yr1eCCRQ4ynWHsctbEb7ySHrSynBu0bxupU/edit?usp=sharing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "Creds= pickle.load(open('/home/ubuntu/Vault/Creds.pkl','rb'))\n",
      "print Creds.keys()\n",
      "print Creds['mrjob'].keys()\n",
      "pair=Creds['mrjob']\n",
      "key_id=pair['key_id']\n",
      "secret_key=pair['secret_key']\n",
      "ID=pair['ID']\n",
      "print ID,key_id"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IOError",
       "evalue": "[Errno 2] No such file or directory: '/home/ubuntu/Vault/Creds.pkl'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-7-4b9c79b90b32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mCreds\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/home/ubuntu/Vault/Creds.pkl'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mCreds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mCreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mrjob'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpair\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mrjob'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/home/ubuntu/Vault/Creds.pkl'"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Checking for an available job flow###\n",
      "Before submitting your job for execution you need to find out which job flows are active and waiting. THe following cell will do that for you. If there is a waiting cluster, it will put it's ID into the variable `job_flow_id`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "job_flow_id=find_waiting_flow(key_id,secret_key)\n",
      "job_flow_id"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<boto.emr.emrobject.JobFlow object at 0x535d390> no_script.yoavfreund.20140517.080731.371759 j-1RE8D7HBISOI0 WAITING\n",
        "<boto.emr.emrobject.JobFlow object at 0x52b5250> no_script.yoavfreund.20140516.040032.370095 j-262J0JTFJIRLO WAITING\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "u'j-262J0JTFJIRLO'"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Setting up the mrjob configuration file ###\n",
      "The last step before submitting the job is to insert the credentials into the file `/home/ubuntu/UCSD_BigData/utils/mrjob.conf.template` and put the result in the default location for the mrjob configuration file which is: `/home/ubuntu/.mrjob.conf`\n",
      "this is done by the following line. If the return value is `True` you are good to go. If it is `False` something went wrong and you should get an error message.\n",
      "\n",
      "The template file should work as is. However, feel free to change and add fields to this configuration file to make it your own.\n",
      "\n",
      "This step needs to be done just one time. Redone only when starting a new EC2 instance or if the credentials changed.\n",
      "It is better to do it in an interactive shell, rather than in a notebook. Here it is done in the notebook for demonstration purpose."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%cd /home/ubuntu/UCSD_BigData/utils/\n",
      "!python Make.mrjob.conf.py\n",
      "%cd ../notebooks/weather.mapreduce/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/ubuntu/UCSD_BigData/utils\n",
        "[Errno 2] No such file or directory: '/home/ubuntu/Vault/Creds.pkl'\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/ubuntu/UCSD_BigData/notebooks/weather.mapreduce\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Assuming the steps up to here were all successful, you should be ready to launch your mrjob job. There is no need to change anything\n",
      "in the following line."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python mr_weather.py -r emr --emr-job-flow-id  $job_flow_id < $local_data > counts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "using configs in /home/ubuntu/.mrjob.conf\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "creating tmp directory /tmp/mr_weather.ubuntu.20140520.000201.384476\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "reading from STDIN\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Copying non-input files into s3://yoav.hadoop/scratch/mr_weather.ubuntu.20140520.000201.384476/files/\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Adding our job to existing job flow j-262J0JTFJIRLO\r\n"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python mr_weather.py -r emr --emr-job-flow-id $job_flow_id hdfs:/weather/weather.csv > counts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat counts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Two useful command-line utilities"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "the mrjob command line depends on the same configuration file as the run-time library. This configuration file is, by default, located at ~/.mrjob.conf"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!mrjob"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!mrjob audit-emr-usage"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "s3cmd is a utility that makes it easy to work with s3"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!s3cmd --help"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Usage: s3cmd [options] COMMAND [parameters]\r\n",
        "\r\n",
        "S3cmd is a tool for managing objects in Amazon S3 storage. It allows for\r\n",
        "making and removing \"buckets\" and uploading, downloading and removing\r\n",
        "\"objects\" from these buckets.\r\n",
        "\r\n",
        "Options:\r\n",
        "  -h, --help            show this help message and exit\r\n",
        "  --configure           Invoke interactive (re)configuration tool.\r\n",
        "  -c FILE, --config=FILE\r\n",
        "                        Config file name. Defaults to /home/ubuntu/.s3cfg\r\n",
        "  --dump-config         Dump current configuration after parsing config files\r\n",
        "                        and command line options and exit.\r\n",
        "  -n, --dry-run         Only show what should be uploaded or downloaded but\r\n",
        "                        don't actually do it. May still perform S3 requests to\r\n",
        "                        get bucket listings and other information though (only\r\n",
        "                        for file transfer commands)\r\n",
        "  -e, --encrypt         Encrypt files before uploading to S3.\r\n",
        "  --no-encrypt          Don't encrypt files.\r\n",
        "  -f, --force           Force overwrite and other dangerous operations.\r\n",
        "  --continue            Continue getting a partially downloaded file (only for\r\n",
        "                        [get] command).\r\n",
        "  --skip-existing       Skip over files that exist at the destination (only\r\n",
        "                        for [get] and [sync] commands).\r\n",
        "  -r, --recursive       Recursive upload, download or removal.\r\n",
        "  --check-md5           Check MD5 sums when comparing files for [sync].\r\n",
        "                        (default)\r\n",
        "  --no-check-md5        Do not check MD5 sums when comparing files for [sync].\r\n",
        "                        Only size will be compared. May significantly speed up\r\n",
        "                        transfer but may also miss some changed files.\r\n",
        "  -P, --acl-public      Store objects with ACL allowing read for anyone.\r\n",
        "  --acl-private         Store objects with default ACL allowing access for you\r\n",
        "                        only.\r\n",
        "  --acl-grant=PERMISSION:EMAIL or USER_CANONICAL_ID\r\n",
        "                        Grant stated permission to a given amazon user.\r\n",
        "                        Permission is one of: read, write, read_acp,\r\n",
        "                        write_acp, full_control, all\r\n",
        "  --acl-revoke=PERMISSION:USER_CANONICAL_ID\r\n",
        "                        Revoke stated permission for a given amazon user.\r\n",
        "                        Permission is one of: read, write, read_acp, wr\r\n",
        "                        ite_acp, full_control, all\r\n",
        "  --delete-removed      Delete remote objects with no corresponding local file\r\n",
        "                        [sync]\r\n",
        "  --no-delete-removed   Don't delete remote objects.\r\n",
        "  -p, --preserve        Preserve filesystem attributes (mode, ownership,\r\n",
        "                        timestamps). Default for [sync] command.\r\n",
        "  --no-preserve         Don't store FS attributes\r\n",
        "  --exclude=GLOB        Filenames and paths matching GLOB will be excluded\r\n",
        "                        from sync\r\n",
        "  --exclude-from=FILE   Read --exclude GLOBs from FILE\r\n",
        "  --rexclude=REGEXP     Filenames and paths matching REGEXP (regular\r\n",
        "                        expression) will be excluded from sync\r\n",
        "  --rexclude-from=FILE  Read --rexclude REGEXPs from FILE\r\n",
        "  --include=GLOB        Filenames and paths matching GLOB will be included\r\n",
        "                        even if previously excluded by one of\r\n",
        "                        --(r)exclude(-from) patterns\r\n",
        "  --include-from=FILE   Read --include GLOBs from FILE\r\n",
        "  --rinclude=REGEXP     Same as --include but uses REGEXP (regular expression)\r\n",
        "                        instead of GLOB\r\n",
        "  --rinclude-from=FILE  Read --rinclude REGEXPs from FILE\r\n",
        "  --bucket-location=BUCKET_LOCATION\r\n",
        "                        Datacentre to create bucket in. As of now the\r\n",
        "                        datacenters are: US (default), EU, us-west-1, and ap-\r\n",
        "                        southeast-1\r\n",
        "  --reduced-redundancy, --rr\r\n",
        "                        Store object with 'Reduced redundancy'. Lower per-GB\r\n",
        "                        price. [put, cp, mv]\r\n",
        "  --access-logging-target-prefix=LOG_TARGET_PREFIX\r\n",
        "                        Target prefix for access logs (S3 URI) (for [cfmodify]\r\n",
        "                        and [accesslog] commands)\r\n",
        "  --no-access-logging   Disable access logging (for [cfmodify] and [accesslog]\r\n",
        "                        commands)\r\n",
        "  -m MIME/TYPE, --mime-type=MIME/TYPE\r\n",
        "                        Default MIME-type to be set for objects stored.\r\n",
        "  -M, --guess-mime-type\r\n",
        "                        Guess MIME-type of files by their extension. Falls\r\n",
        "                        back to default MIME-Type as specified by --mime-type\r\n",
        "                        option\r\n",
        "  --add-header=NAME:VALUE\r\n",
        "                        Add a given HTTP header to the upload request. Can be\r\n",
        "                        used multiple times. For instance set 'Expires' or\r\n",
        "                        'Cache-Control' headers (or both) using this options\r\n",
        "                        if you like.\r\n",
        "  --encoding=ENCODING   Override autodetected terminal and filesystem encoding\r\n",
        "                        (character set). Autodetected: UTF-8\r\n",
        "  --verbatim            Use the S3 name as given on the command line. No pre-\r\n",
        "                        processing, encoding, etc. Use with caution!\r\n",
        "  --list-md5            Include MD5 sums in bucket listings (only for 'ls'\r\n",
        "                        command).\r\n",
        "  -H, --human-readable-sizes\r\n",
        "                        Print sizes in human readable form (eg 1kB instead of\r\n",
        "                        1234).\r\n",
        "  --progress            Display progress meter (default on TTY).\r\n",
        "  --no-progress         Don't display progress meter (default on non-TTY).\r\n",
        "  --enable              Enable given CloudFront distribution (only for\r\n",
        "                        [cfmodify] command)\r\n",
        "  --disable             Enable given CloudFront distribution (only for\r\n",
        "                        [cfmodify] command)\r\n",
        "  --cf-add-cname=CNAME  Add given CNAME to a CloudFront distribution (only for\r\n",
        "                        [cfcreate] and [cfmodify] commands)\r\n",
        "  --cf-remove-cname=CNAME\r\n",
        "                        Remove given CNAME from a CloudFront distribution\r\n",
        "                        (only for [cfmodify] command)\r\n",
        "  --cf-comment=COMMENT  Set COMMENT for a given CloudFront distribution (only\r\n",
        "                        for [cfcreate] and [cfmodify] commands)\r\n",
        "  --cf-default-root-object=DEFAULT_ROOT_OBJECT\r\n",
        "                        Set the default root object to return when no object\r\n",
        "                        is specified in the URL. Use a relative path, i.e.\r\n",
        "                        default/index.html instead of /default/index.html or\r\n",
        "                        s3://bucket/default/index.html (only for [cfcreate]\r\n",
        "                        and [cfmodify] commands)\r\n",
        "  -v, --verbose         Enable verbose output.\r\n",
        "  -d, --debug           Enable debug output.\r\n",
        "  --version             Show s3cmd version (1.0.0) and exit.\r\n",
        "  -F, --follow-symlinks\r\n",
        "                        Follow symbolic links as if they are regular files\r\n",
        "\r\n",
        "Commands:\r\n",
        "  Make bucket\r\n",
        "      s3cmd mb s3://BUCKET\r\n",
        "  Remove bucket\r\n",
        "      s3cmd rb s3://BUCKET\r\n",
        "  List objects or buckets\r\n",
        "      s3cmd ls [s3://BUCKET[/PREFIX]]\r\n",
        "  List all object in all buckets\r\n",
        "      s3cmd la \r\n",
        "  Put file into bucket\r\n",
        "      s3cmd put FILE [FILE...] s3://BUCKET[/PREFIX]\r\n",
        "  Get file from bucket\r\n",
        "      s3cmd get s3://BUCKET/OBJECT LOCAL_FILE\r\n",
        "  Delete file from bucket\r\n",
        "      s3cmd del s3://BUCKET/OBJECT\r\n",
        "  Synchronize a directory tree to S3\r\n",
        "      s3cmd sync LOCAL_DIR s3://BUCKET[/PREFIX] or s3://BUCKET[/PREFIX] LOCAL_DIR\r\n",
        "  Disk usage by buckets\r\n",
        "      s3cmd du [s3://BUCKET[/PREFIX]]\r\n",
        "  Get various information about Buckets or Files\r\n",
        "      s3cmd info s3://BUCKET[/OBJECT]\r\n",
        "  Copy object\r\n",
        "      s3cmd cp s3://BUCKET1/OBJECT1 s3://BUCKET2[/OBJECT2]\r\n",
        "  Move object\r\n",
        "      s3cmd mv s3://BUCKET1/OBJECT1 s3://BUCKET2[/OBJECT2]\r\n",
        "  Modify Access control list for Bucket or Files\r\n",
        "      s3cmd setacl s3://BUCKET[/OBJECT]\r\n",
        "  Enable/disable bucket access logging\r\n",
        "      s3cmd accesslog s3://BUCKET\r\n",
        "  Sign arbitrary string using the secret key\r\n",
        "      s3cmd sign STRING-TO-SIGN\r\n",
        "  Fix invalid file names in a bucket\r\n",
        "      s3cmd fixbucket s3://BUCKET[/PREFIX]\r\n",
        "  List CloudFront distribution points\r\n",
        "      s3cmd cflist \r\n",
        "  Display CloudFront distribution point parameters\r\n",
        "      s3cmd cfinfo [cf://DIST_ID]\r\n",
        "  Create CloudFront distribution point\r\n",
        "      s3cmd cfcreate s3://BUCKET\r\n",
        "  Delete CloudFront distribution point\r\n",
        "      s3cmd cfdelete cf://DIST_ID\r\n",
        "  Change CloudFront distribution point parameters\r\n",
        "      s3cmd cfmodify cf://DIST_ID\r\n",
        "\r\n",
        "For more informations see the progect homepage:\r\n",
        "http://s3tools.org\r\n",
        "\r\n",
        "Consider a donation if you have found s3cmd useful:\r\n",
        "http://s3tools.org/donate\r\n",
        "\r\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    }
   ],
   "metadata": {}
  }
 ]
}